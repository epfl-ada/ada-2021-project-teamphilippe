{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "basic_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "DpBF2QlZCaBF",
        "6f88b083",
        "206ceca9",
        "v_0HRt9r4nQP",
        "K9aJyhq8B3vP",
        "Tb_j4Gy70Ltm"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpBF2QlZCaBF"
      },
      "source": [
        "# Import and Drive mount"
      ],
      "id": "DpBF2QlZCaBF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQpdfVxTvVgU"
      },
      "source": [
        "Note that if you want to run our notebook, you will first need to send us your email so that we can add you on our shared drive containing the data for our project. Furthermore, when running the cell starting with !pip install, if you have an earlier version of pandas, you will need to restart the kernel before running the cells and ckeck that the pandas version is at least `1.3.4`"
      ],
      "id": "WQpdfVxTvVgU"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcIQDG6DoZOX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "ec57c7a2-0548-4215-bafd-b854e2dff7c2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "WcIQDG6DoZOX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7VFdwhOzUnu",
        "collapsed": true,
        "outputId": "0eacf55b-f7e7-41db-cc5a-c724ad9648f2"
      },
      "source": [
        "!pip install pandas==1.3.4\n",
        "import pandas as pd\n",
        "print()\n",
        "pd.__version__"
      ],
      "id": "b7VFdwhOzUnu",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas==1.3.4 in /usr/local/lib/python3.7/dist-packages (1.3.4)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.4) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.4) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.4) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas==1.3.4) (1.15.0)\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.3.4'"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "852f7d7f"
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import bz2\n",
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import pyarrow.parquet as pq\n",
        "from datetime import datetime \n",
        "import seaborn as sns"
      ],
      "id": "852f7d7f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f88b083"
      },
      "source": [
        "# Utility function and paths variables"
      ],
      "id": "6f88b083"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0a5eea3"
      },
      "source": [
        "def apply_to_stream(f, input_file, args=None, chunksize=1_000_000):\n",
        "  \"\"\"\n",
        "    apply func to input_file and save in output_file\n",
        "\n",
        "    Parameters:\n",
        "      func :        fonction to apply of type : args = func(df_chunk, args)\n",
        "      input_file :  input file\n",
        "      args :        arguments of func \n",
        "      chunksize :   chunk size\n",
        "\n",
        "    Returns:\n",
        "      args :        arguments at the end of the execution\n",
        "  \"\"\"\n",
        "  with pd.read_json(input_file, lines=True, compression='bz2', chunksize=chunksize) as df_reader:\n",
        "    for chunk in tqdm(df_reader):\n",
        "      args = f(chunk, args)\n",
        "  return args\n",
        "\n",
        "def write_df_chunk_to_file(df_chunk, file):\n",
        "  \"\"\"\n",
        "    write the chunk in to the file\n",
        "\n",
        "    Parameters:\n",
        "      df_chunk :  fonction to apply of type : new_df_chunk, args = func(df_chunk, args)\n",
        "      file :      file_stream\n",
        "  \"\"\"\n",
        "  file.write(df_chunk.to_json(orient='records', lines=True).encode('utf-8'))\n",
        "\n",
        "def apply_to_stream_and_save(func, input_file, output_file, args=None, chunksize=1_000_000, override=False):\n",
        "  \"\"\"\n",
        "    apply func to input_file and save in output_file\n",
        "\n",
        "    Parameters:\n",
        "      func :        fonction to apply of type : new_df_chunk, args = func(df_chunk, args)\n",
        "      input_file :  input file\n",
        "      output_file : output file\n",
        "      args :        arguments of func \n",
        "      chunksize :   chunk size\n",
        "      override :    override output_file if alredy exist\n",
        "\n",
        "    Returns:\n",
        "      args :        arguments at the end of the execution\n",
        "  \"\"\"\n",
        "  if(override or (isfile(input_file) and not isfile(output_file))):\n",
        "    with pd.read_json(input_file, lines=True, compression='bz2', chunksize=chunksize) as df_reader:\n",
        "        with bz2.open(output_file, 'wb') as out_file:\n",
        "            for df_chunk in tqdm(df_reader):\n",
        "              new_df_chunk, args = func(df_chunk, args)\n",
        "              write_df_chunk_to_file(new_df_chunk, out_file)\n",
        "  return args\n",
        "\n",
        "\n",
        "def apply_to_all_stream_and_save(func, all_input_file, output_file, args=None, chunksize=1_000_000, override=False):\n",
        "  \"\"\"\n",
        "    apply func to all input_file and save in output_file\n",
        "\n",
        "    Parameters:\n",
        "      func :            fonction to apply of type : new_df_chunk, args = func(df_chunk, args)\n",
        "      all_input_file :  input file\n",
        "      output_file :     output file\n",
        "      args :            arguments of func \n",
        "      chunksize :       chunk size\n",
        "      override :        override output_file if alredy exist\n",
        "\n",
        "    Returns:\n",
        "      args :        arguments at the end of the execution\n",
        "  \"\"\"\n",
        "  if(override or not isfile(output_file)):\n",
        "    # Open only once the output file, so just append all the content to it\n",
        "    with bz2.open(output_file, 'wb') as out_file:\n",
        "      for key in all_input_file:\n",
        "        input_file = all_input_file[key]\n",
        "        print(f'==> File \"{input_file}\" start')\n",
        "        with pd.read_json(input_file, lines=True, compression='bz2', chunksize=chunksize) as df_reader:\n",
        "          for df_chunk in tqdm(df_reader):\n",
        "            new_df_chunk, args = func(df_chunk, args)\n",
        "            write_df_chunk_to_file(new_df_chunk, out_file)\n",
        "                        \n",
        "        print(f'==> File \"{input_file}\" processed')\n",
        "  return args\n",
        "\n",
        "def filter_file_and_save(func, input_file, output_file, name='', chunksize=1_000_000, override=False):\n",
        "  \"\"\"\n",
        "    filter input_file and save in output_file\n",
        "\n",
        "    Parameters:\n",
        "      func :        fonction to apply of type : new_df_chunk = func(df_chunk)\n",
        "      input_file :  input file\n",
        "      output_file : output file\n",
        "      chunksize :   chunk size\n",
        "      override :    override output_file if alredy exist\n",
        "  \"\"\"\n",
        "  if(override or (isfile(input_file) and not isfile(output_file))):\n",
        "    def f(df, total_nb_rows):\n",
        "      new_df = func(df)\n",
        "      return new_df, (total_nb_rows + df.shape[0] - new_df.shape[0])\n",
        "    total_nb_rows = apply_to_stream_and_save(f, input_file, output_file, 0, chunksize)\n",
        "    print(f'Number of rows dropped {name} : {total_nb_rows}')\n",
        "\n",
        "def count_nb_rows_in_file(input_file):\n",
        "  \"\"\"\n",
        "    count nb rows in input_file\n",
        "\n",
        "    Parameters:\n",
        "      input_file :  input file\n",
        "  \"\"\"\n",
        "  count = 0\n",
        "  with pd.read_json(input_file, lines=True, compression='bz2', chunksize=750_000) as df_reader:\n",
        "      for chunk in df_reader:\n",
        "          count += chunk.shape[0]\n",
        "\n",
        "  print(f'Total number of records in {input_file}: {count}')"
      ],
      "id": "c0a5eea3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5a4ea8f"
      },
      "source": [
        "drive = '/content/drive/Shareddrives/ada-teamphilippe/'\n",
        "\n",
        "# Source Data\n",
        "wikidata_folder = drive + 'Wikidata/'\n",
        "quotebank_folder = drive + 'Quotebank/'\n",
        "\n",
        "wikidata_speaker = wikidata_folder + 'speaker_attributes.parquet/'\n",
        "wikidata_labels = wikidata_folder + 'wikidata_labels_descriptions.csv.bz2'\n",
        "wikidata_labels_quotebank = wikidata_folder + 'wikidata_labels_descriptions_quotebank.csv.bz2'\n",
        "wikidata_speaker_parquet = [join(wikidata_speaker, f) for f in listdir(wikidata_speaker) if (isfile(join(wikidata_speaker, f) )and f.endswith('.parquet'))]\n",
        "\n",
        "# Prosesed Data\n",
        "data_out_folder = drive + 'data/'\n",
        "\n",
        "cleaned_quotes_folder = data_out_folder + 'cleaned_quotes/'\n",
        "speaker_quotes_folder = data_out_folder + 'speaker_quotes/'\n",
        "sampled_quotes_folder = data_out_folder + 'sampled_quotes/'\n",
        "sampled_speaker_quotes_folder = data_out_folder + 'sampled_speaker_quotes/'\n",
        "\n",
        "# Files\n",
        "quotes_name = {}\n",
        "quotes = {}\n",
        "\n",
        "cleaned_quotes = {}\n",
        "speaker_quotes = {}\n",
        "sampled_quotes = {}\n",
        "sampled_speaker_quotes = {}\n",
        "\n",
        "for year in range(2015, 2020 + 1):\n",
        "  quotes_name[year] = f'quotes-{year}.json.bz2'\n",
        "  quotes[year] = quotebank_folder + quotes_name[year]\n",
        "\n",
        "  cleaned_quotes[year] = cleaned_quotes_folder + quotes_name[year]\n",
        "  speaker_quotes[year] = speaker_quotes_folder + quotes_name[year]\n",
        "  sampled_quotes[year] = sampled_quotes_folder + quotes_name[year]\n",
        "  sampled_speaker_quotes[year] = sampled_speaker_quotes_folder + quotes_name[year]\n",
        "\n",
        "sampled_quotes_all = data_out_folder + 'sampled_quotes_all.json.bz2'\n",
        "sampled_speaker_quotes_all = data_out_folder + 'sampled_speaker_quotes_all.json.bz2'"
      ],
      "id": "c5a4ea8f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAWXLwXRCrWS"
      },
      "source": [
        "# Quotebank dataset formats, cleaning, compliting and sampling"
      ],
      "id": "qAWXLwXRCrWS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "936d0d45"
      },
      "source": [
        "## Formats of the differents features"
      ],
      "id": "936d0d45"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46785a3e"
      },
      "source": [
        "def get(quotes, nb):\n",
        "  \"\"\"\n",
        "    get first chunk of quotes of size nb\n",
        "\n",
        "    Parameters:\n",
        "      quotes :  quotes file name\n",
        "      nb :      number of rows\n",
        "\n",
        "    Returns:\n",
        "      chunk :   chunk of the file\n",
        "  \"\"\"\n",
        "  with pd.read_json(quotes, lines=True, compression='bz2', chunksize=nb) as df_reader:\n",
        "    for chunk in df_reader:\n",
        "      return chunk"
      ],
      "id": "46785a3e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2be4303a",
        "collapsed": true
      },
      "source": [
        "first_chunk = get(quotes[2015], 50)"
      ],
      "id": "2be4303a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aac73c89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8529b5bb-411f-442b-8120-adc4de894e98"
      },
      "source": [
        "first_chunk.head(50)"
      ],
      "id": "aac73c89",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>quoteID</th>\n",
              "      <th>quotation</th>\n",
              "      <th>speaker</th>\n",
              "      <th>qids</th>\n",
              "      <th>date</th>\n",
              "      <th>numOccurrences</th>\n",
              "      <th>probas</th>\n",
              "      <th>urls</th>\n",
              "      <th>phase</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2015-08-31-000271</td>\n",
              "      <td>... a great day for veterans here in Littleton...</td>\n",
              "      <td>Jeanne Shaheen</td>\n",
              "      <td>[Q270316]</td>\n",
              "      <td>2015-08-31 02:10:00</td>\n",
              "      <td>2</td>\n",
              "      <td>[[Jeanne Shaheen, 0.742], [None, 0.2359], [Kel...</td>\n",
              "      <td>[http://www.unionleader.com/article/20150831/N...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2015-12-08-029916</td>\n",
              "      <td>How FFA scored 32 own goals in 18 months and C...</td>\n",
              "      <td>None</td>\n",
              "      <td>[]</td>\n",
              "      <td>2015-12-08 00:00:00</td>\n",
              "      <td>2</td>\n",
              "      <td>[[None, 0.563], [David Gallop, 0.437]]</td>\n",
              "      <td>[http://feeds.theroar.com.au/~r/theroar/~3/tZ3...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2015-09-10-000206</td>\n",
              "      <td>[ Amy ] was placed under an unacceptable amoun...</td>\n",
              "      <td>None</td>\n",
              "      <td>[]</td>\n",
              "      <td>2015-09-10 21:18:59</td>\n",
              "      <td>1</td>\n",
              "      <td>[[None, 0.9634], [Amy Robinson, 0.0366]]</td>\n",
              "      <td>[http://www.thefashionspot.com/buzz-news/lates...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2015-07-23-032707</td>\n",
              "      <td>How High Will These Numbers Go?</td>\n",
              "      <td>None</td>\n",
              "      <td>[]</td>\n",
              "      <td>2015-07-23 14:57:49</td>\n",
              "      <td>3</td>\n",
              "      <td>[[None, 0.9019], [Chubby Checker, 0.0981]]</td>\n",
              "      <td>[http://www.billboard.com/node/6641719, http:/...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2015-10-04-000115</td>\n",
              "      <td>[ Ban ] recalls that hospitals and medical per...</td>\n",
              "      <td>Ban Ki-moon</td>\n",
              "      <td>[Q1253]</td>\n",
              "      <td>2015-10-04 08:27:38</td>\n",
              "      <td>2</td>\n",
              "      <td>[[Ban Ki-moon, 0.8399], [None, 0.1601]]</td>\n",
              "      <td>[http://muslimnews.co.uk/news/middle-east/afgh...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2015-06-19-016191</td>\n",
              "      <td>How Indian values and karma can help business ...</td>\n",
              "      <td>Sri Sri Ravi Shankar</td>\n",
              "      <td>[Q468374]</td>\n",
              "      <td>2015-06-19 15:32:16</td>\n",
              "      <td>1</td>\n",
              "      <td>[[Sri Sri Ravi Shankar, 0.4169], [None, 0.2844...</td>\n",
              "      <td>[http://newkerala.com/news/2015/fullnews-74877...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2015-08-31-000309</td>\n",
              "      <td>[ But ] We were able to network within our com...</td>\n",
              "      <td>Jamal Rifi</td>\n",
              "      <td>[Q19874690]</td>\n",
              "      <td>2015-08-31 22:59:36</td>\n",
              "      <td>1</td>\n",
              "      <td>[[Jamal Rifi, 0.6109], [None, 0.3891]]</td>\n",
              "      <td>[http://sbs.com.au/content/muslim-leader-named...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2015-09-21-030618</td>\n",
              "      <td>How it worked is that they had a keyboard and ...</td>\n",
              "      <td>Richard Burmeister</td>\n",
              "      <td>[Q18601741]</td>\n",
              "      <td>2015-09-21 19:00:00</td>\n",
              "      <td>1</td>\n",
              "      <td>[[Richard Burmeister, 0.875], [None, 0.125]]</td>\n",
              "      <td>[http://sarahrosedevilliers.wordpress.com/2015...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2015-12-10-000332</td>\n",
              "      <td>[ C ] harter operators working through third-p...</td>\n",
              "      <td>Diane Ravitch</td>\n",
              "      <td>[Q5271548]</td>\n",
              "      <td>2015-12-10 17:51:45</td>\n",
              "      <td>1</td>\n",
              "      <td>[[Diane Ravitch, 0.8078], [None, 0.1922]]</td>\n",
              "      <td>[http://enewspf.com/2015/12/10/report-shows-ho...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2015-06-18-018819</td>\n",
              "      <td>How long do you think it'll take for Donald Tr...</td>\n",
              "      <td>Chris Matthews</td>\n",
              "      <td>[Q15735939, Q25189328, Q5107375, Q5110828, Q51...</td>\n",
              "      <td>2015-06-18 10:00:00</td>\n",
              "      <td>1</td>\n",
              "      <td>[[Chris Matthews, 0.3869], [None, 0.3198], [Do...</td>\n",
              "      <td>[http://talkingpointsmemo.com/dc/donald-trump-...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2015-08-21-000405</td>\n",
              "      <td>... Current EHRs are built to fulfill data exc...</td>\n",
              "      <td>Steven Stack</td>\n",
              "      <td>[Q51797519]</td>\n",
              "      <td>2015-08-21 14:01:17</td>\n",
              "      <td>1</td>\n",
              "      <td>[[Steven Stack, 0.7309], [None, 0.2381], [Paul...</td>\n",
              "      <td>[http://www.politico.com/morningehealth/0815/m...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2015-09-13-014399</td>\n",
              "      <td>How long is a crocodile's tail?</td>\n",
              "      <td>None</td>\n",
              "      <td>[]</td>\n",
              "      <td>2015-09-13 23:30:38</td>\n",
              "      <td>1</td>\n",
              "      <td>[[None, 0.8273], [Jeanne Cavelos, 0.1727]]</td>\n",
              "      <td>[http://odysseyworkshop.wordpress.com/2015/09/...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2015-09-16-000331</td>\n",
              "      <td>[ Day and Fowler ] are good buddies. I'm frien...</td>\n",
              "      <td>Jordan Spieth</td>\n",
              "      <td>[Q2287947]</td>\n",
              "      <td>2015-09-16 15:56:00</td>\n",
              "      <td>1</td>\n",
              "      <td>[[Jordan Spieth, 0.7243], [None, 0.2333], [Ror...</td>\n",
              "      <td>[http://chicago.suntimes.com/golf-sports/7/71/...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2015-12-17-031881</td>\n",
              "      <td>How many Medals of Honor have Delta guys gotte...</td>\n",
              "      <td>Tom Brady</td>\n",
              "      <td>[Q2376327, Q313381, Q7815037]</td>\n",
              "      <td>2015-12-17 18:36:07</td>\n",
              "      <td>1</td>\n",
              "      <td>[[Tom Brady, 0.8382], [None, 0.1263], [Joshua ...</td>\n",
              "      <td>[http://esquire.com/news-politics/a40361/the-o...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2015-12-15-000241</td>\n",
              "      <td>' Encore! Encore!' auction items are perfect g...</td>\n",
              "      <td>None</td>\n",
              "      <td>[]</td>\n",
              "      <td>2015-12-15 18:43:17</td>\n",
              "      <td>1</td>\n",
              "      <td>[[None, 0.9155], [Richard Rodgers, 0.0845]]</td>\n",
              "      <td>[http://news.hamlethub.com/fairfield/neighbors...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2015-10-29-035215</td>\n",
              "      <td>how the greatest movement came into being and ...</td>\n",
              "      <td>None</td>\n",
              "      <td>[]</td>\n",
              "      <td>2015-10-29 21:25:14</td>\n",
              "      <td>1</td>\n",
              "      <td>[[None, 0.5458], [Paul Hawken, 0.4542]]</td>\n",
              "      <td>[https://medium.com/enspiral-tales/bioneers-da...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2015-12-21-000213</td>\n",
              "      <td>... He is a playmaker and we are very fortunat...</td>\n",
              "      <td>Mike McCoy</td>\n",
              "      <td>[Q16270895, Q16732257, Q19666929, Q3313441, Q3...</td>\n",
              "      <td>2015-12-21 03:15:00</td>\n",
              "      <td>1</td>\n",
              "      <td>[[Mike McCoy, 0.6875], [None, 0.297], [Melvin ...</td>\n",
              "      <td>[http://www.ocregister.com/articles/yards-6969...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2015-12-17-031947</td>\n",
              "      <td>How the July 7, 2005, London Bombings Affected...</td>\n",
              "      <td>None</td>\n",
              "      <td>[]</td>\n",
              "      <td>2015-12-17 05:03:27</td>\n",
              "      <td>1</td>\n",
              "      <td>[[None, 0.5459], [Diane M. Houston, 0.3165], [...</td>\n",
              "      <td>[http://www.eurekalert.org/pub_releases/2015-1...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>2015-08-28-000528</td>\n",
              "      <td>' I am a journalist and my job is to ask quest...</td>\n",
              "      <td>None</td>\n",
              "      <td>[]</td>\n",
              "      <td>2015-08-28 06:01:19</td>\n",
              "      <td>1</td>\n",
              "      <td>[[None, 0.7489], [Jorge Ramos, 0.2511]]</td>\n",
              "      <td>[http://www.examiner.com/article/audio-suggest...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>2015-09-25-029553</td>\n",
              "      <td>how to rearrange all the systems that are havi...</td>\n",
              "      <td>Alison Holcomb</td>\n",
              "      <td>[Q29578120]</td>\n",
              "      <td>2015-09-25 23:20:00</td>\n",
              "      <td>1</td>\n",
              "      <td>[[Alison Holcomb, 0.8543], [None, 0.1001], [Mi...</td>\n",
              "      <td>[http://stltoday.com/news/local/crime-and-cour...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>2015-08-25-000543</td>\n",
              "      <td>' I have a boyfriend! I can't be here! '</td>\n",
              "      <td>Lara Flynn Boyle</td>\n",
              "      <td>[Q486103]</td>\n",
              "      <td>2015-08-25 23:53:29</td>\n",
              "      <td>1</td>\n",
              "      <td>[[Lara Flynn Boyle, 0.7931], [None, 0.0988], [...</td>\n",
              "      <td>[http://www.complex.com/pop-culture/2015/08/ja...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>2015-03-22-011043</td>\n",
              "      <td>How we have not come away with three points is...</td>\n",
              "      <td>Chris Ramsey</td>\n",
              "      <td>[Q5107822, Q5107823]</td>\n",
              "      <td>2015-03-22 19:40:43</td>\n",
              "      <td>1</td>\n",
              "      <td>[[Chris Ramsey, 0.9699], [None, 0.0296], [Juni...</td>\n",
              "      <td>[http://www.soccerway.com/news/2015/March/22/q...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>2015-12-13-000405</td>\n",
              "      <td>... I try to feel the guy's leverage and see i...</td>\n",
              "      <td>Jordan Reed</td>\n",
              "      <td>[Q6276908]</td>\n",
              "      <td>2015-12-13 07:49:00</td>\n",
              "      <td>1</td>\n",
              "      <td>[[Jordan Reed, 0.8354], [None, 0.1646]]</td>\n",
              "      <td>[http://feeds.washingtonpost.com/c/34656/f/645...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>2015-03-25-025189</td>\n",
              "      <td>how were you able to get that cat to do what y...</td>\n",
              "      <td>Cate Blanchett</td>\n",
              "      <td>[Q80966]</td>\n",
              "      <td>2015-03-25 21:26:54</td>\n",
              "      <td>1</td>\n",
              "      <td>[[Cate Blanchett, 0.6709], [None, 0.3291]]</td>\n",
              "      <td>[http://www.pedestrian.tv/news/arts-and-cultur...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>2015-09-30-000567</td>\n",
              "      <td>&amp; iexcl; Vivos los queremos!</td>\n",
              "      <td>None</td>\n",
              "      <td>[]</td>\n",
              "      <td>2015-09-30 13:31:51</td>\n",
              "      <td>1</td>\n",
              "      <td>[[None, 0.8361], [Jes√∫s Murillo Karam, 0.1639]]</td>\n",
              "      <td>[http://www.newyorker.com/news/news-desk/mexic...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>2015-03-08-011997</td>\n",
              "      <td>How would you describe your dad in three words.</td>\n",
              "      <td>Stephen Mulhern</td>\n",
              "      <td>[Q381543]</td>\n",
              "      <td>2015-03-08 06:11:56</td>\n",
              "      <td>3</td>\n",
              "      <td>[[Stephen Mulhern, 0.8152], [None, 0.1095], [J...</td>\n",
              "      <td>[http://sunderlandecho.com/what-s-on/cinema-tv...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>2015-10-25-000242</td>\n",
              "      <td>' It is not now, nor has it ever been, the gol...</td>\n",
              "      <td>Bernie Sanders</td>\n",
              "      <td>[Q359442]</td>\n",
              "      <td>2015-10-25 14:12:35</td>\n",
              "      <td>1</td>\n",
              "      <td>[[Bernie Sanders, 0.5395], [None, 0.3128], [Hi...</td>\n",
              "      <td>[http://examiner.com/article/bernie-sanders-sl...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>2015-04-06-017849</td>\n",
              "      <td>How would you design that computer? And what n...</td>\n",
              "      <td>None</td>\n",
              "      <td>[]</td>\n",
              "      <td>2015-04-06 15:50:59</td>\n",
              "      <td>2</td>\n",
              "      <td>[[None, 0.8049], [James Hendler, 0.1951]]</td>\n",
              "      <td>[http://news.rpi.edu/content/2015/04/06/neurom...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>2015-07-06-000009</td>\n",
              "      <td>[ It's ] just pure elation, I'm so, so proud o...</td>\n",
              "      <td>None</td>\n",
              "      <td>[]</td>\n",
              "      <td>2015-07-06 18:52:00</td>\n",
              "      <td>1</td>\n",
              "      <td>[[None, 0.4959], [Carli Lloyd, 0.4881], [Abby ...</td>\n",
              "      <td>[http://news.health.com/2015/07/06/10-inspirin...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>2015-04-14-023773</td>\n",
              "      <td>how would you like your salary printed in the ...</td>\n",
              "      <td>Brendan Schwab</td>\n",
              "      <td>[Q16204899]</td>\n",
              "      <td>2015-04-14 22:54:41</td>\n",
              "      <td>1</td>\n",
              "      <td>[[Brendan Schwab, 0.6579], [None, 0.3421]]</td>\n",
              "      <td>[http://brisbanetimes.com.au/rugby-league/leag...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>2015-08-17-000450</td>\n",
              "      <td>' King for a Day' is one of our favorite progr...</td>\n",
              "      <td>Andy Dalton</td>\n",
              "      <td>[Q129605, Q2849092]</td>\n",
              "      <td>2015-08-17 12:44:21</td>\n",
              "      <td>1</td>\n",
              "      <td>[[Andy Dalton, 0.8515], [None, 0.1485]]</td>\n",
              "      <td>[http://700wlw.com/onair/lance-mcalister-7818/...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>2015-04-25-012511</td>\n",
              "      <td>Howard is throwing people all over the place. ...</td>\n",
              "      <td>Rick Carlisle</td>\n",
              "      <td>[Q339256]</td>\n",
              "      <td>2015-04-25 02:05:00</td>\n",
              "      <td>35</td>\n",
              "      <td>[[Rick Carlisle, 0.5329], [None, 0.4288], [Dwi...</td>\n",
              "      <td>[http://www.wsfa.com/story/28895752/harden-how...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>2015-10-23-000509</td>\n",
              "      <td>[ Krzyzewski ] didn't want to deal with it. He...</td>\n",
              "      <td>Rasheed Sulaimon</td>\n",
              "      <td>[Q7294850]</td>\n",
              "      <td>2015-10-23 21:09:34</td>\n",
              "      <td>2</td>\n",
              "      <td>[[Rasheed Sulaimon, 0.8982], [None, 0.1018]]</td>\n",
              "      <td>[http://www.si.com/college-basketball/2015/10/...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>2015-10-04-014109</td>\n",
              "      <td>However, Arsenal did beat United in the FA Cup...</td>\n",
              "      <td>Michael Owen</td>\n",
              "      <td>[Q1061908, Q128829, Q17198421, Q6833276]</td>\n",
              "      <td>2015-10-04 00:30:11</td>\n",
              "      <td>1</td>\n",
              "      <td>[[Michael Owen, 0.8887], [None, 0.1113]]</td>\n",
              "      <td>[http://businessdayonline.com/2015/10/can-man-...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>2015-10-02-000466</td>\n",
              "      <td>[ Miller ] probably has the quickest first ste...</td>\n",
              "      <td>Norv Turner</td>\n",
              "      <td>[Q1758640]</td>\n",
              "      <td>2015-10-02 02:48:40</td>\n",
              "      <td>1</td>\n",
              "      <td>[[Norv Turner, 0.8836], [None, 0.1144], [Matt ...</td>\n",
              "      <td>[http://www.startribune.com/sports/vikings/330...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>2015-10-04-014110</td>\n",
              "      <td>However, as far as Kashmir is concerned, the s...</td>\n",
              "      <td>Manohar Parrikar</td>\n",
              "      <td>[Q1391309]</td>\n",
              "      <td>2015-10-04 14:01:40</td>\n",
              "      <td>6</td>\n",
              "      <td>[[Manohar Parrikar, 0.7398], [None, 0.2049], [...</td>\n",
              "      <td>[http://aninews.in/newsdetail2/story236079/sia...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>2015-11-13-000427</td>\n",
              "      <td>[ My dad ] saw what a great opportunity it was,</td>\n",
              "      <td>Paul Stanton</td>\n",
              "      <td>[Q255668, Q27842994]</td>\n",
              "      <td>2015-11-13 08:45:01</td>\n",
              "      <td>1</td>\n",
              "      <td>[[Paul Stanton, 0.6521], [None, 0.3479]]</td>\n",
              "      <td>[http://thecrimson.com/article/2015/11/13/paul...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>2015-07-21-029023</td>\n",
              "      <td>However, at the moment we haven't received an ...</td>\n",
              "      <td>Horst Heldt</td>\n",
              "      <td>[Q63088]</td>\n",
              "      <td>2015-07-21 19:49:54</td>\n",
              "      <td>10</td>\n",
              "      <td>[[Horst Heldt, 0.8546], [None, 0.1258], [Julia...</td>\n",
              "      <td>[http://www.goal.com/en/news/11/transfer-zone/...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>2015-07-22-000568</td>\n",
              "      <td>[ My granny ] was always very well put together,</td>\n",
              "      <td>Simone Rocha</td>\n",
              "      <td>[Q42969376]</td>\n",
              "      <td>2015-07-22 15:17:00</td>\n",
              "      <td>2</td>\n",
              "      <td>[[Simone Rocha, 0.9306], [None, 0.0694]]</td>\n",
              "      <td>[http://www.dazeddigital.com/fashion/article/2...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>2015-12-31-019814</td>\n",
              "      <td>However, due to poor record keeping and poor m...</td>\n",
              "      <td>Tomas Campos</td>\n",
              "      <td>[Q7820451]</td>\n",
              "      <td>2015-12-31 17:05:06</td>\n",
              "      <td>1</td>\n",
              "      <td>[[Tomas Campos, 0.6024], [None, 0.3976]]</td>\n",
              "      <td>[http://riograndesun.com/articles/2016/01/01/n...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>2015-08-17-000497</td>\n",
              "      <td>[ Or ] `the gorilla played tag with me' which ...</td>\n",
              "      <td>None</td>\n",
              "      <td>[]</td>\n",
              "      <td>2015-08-17 20:10:30</td>\n",
              "      <td>1</td>\n",
              "      <td>[[None, 0.6615], [Andrew Wright, 0.3385]]</td>\n",
              "      <td>[http://www.informationng.com/2015/08/new-zeal...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>2015-03-26-025269</td>\n",
              "      <td>However, due to sharp decline in KG-D6 gas pro...</td>\n",
              "      <td>Piyush Goyal</td>\n",
              "      <td>[Q7199798]</td>\n",
              "      <td>2015-03-26 10:02:46</td>\n",
              "      <td>1</td>\n",
              "      <td>[[Piyush Goyal, 0.9096], [None, 0.0904]]</td>\n",
              "      <td>[http://timesofindia.indiatimes.com/business/i...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>2015-08-27-000768</td>\n",
              "      <td>[ Scott ] was a leader and he was mature,</td>\n",
              "      <td>George Hill</td>\n",
              "      <td>[Q16105952, Q18671272, Q28057479, Q517675, Q55...</td>\n",
              "      <td>2015-08-27 04:49:19</td>\n",
              "      <td>1</td>\n",
              "      <td>[[George Hill, 0.81], [None, 0.19]]</td>\n",
              "      <td>[http://vindy.com/news/2015/aug/27/top-of-the-...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>2015-10-23-031166</td>\n",
              "      <td>However, I was worried when I saw the name bec...</td>\n",
              "      <td>None</td>\n",
              "      <td>[]</td>\n",
              "      <td>2015-10-23 23:18:07</td>\n",
              "      <td>2</td>\n",
              "      <td>[[None, 0.9483], [Attahiru Jega, 0.0517]]</td>\n",
              "      <td>[http://www.punchng.com/politics/bayelsa-kogi-...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>2015-12-01-000551</td>\n",
              "      <td>[ She has used ] her role as Chair of the Hous...</td>\n",
              "      <td>Matthew Flinders</td>\n",
              "      <td>[Q6790508]</td>\n",
              "      <td>2015-12-01 20:00:00</td>\n",
              "      <td>1</td>\n",
              "      <td>[[Matthew Flinders, 0.3311], [Michael Crick, 0...</td>\n",
              "      <td>[http://www.westernmorningnews.co.uk/Devon-MP-...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>2015-06-02-000948</td>\n",
              "      <td>However, in the meantime compulsory encryption...</td>\n",
              "      <td>Chris McIntosh</td>\n",
              "      <td>[Q5107420]</td>\n",
              "      <td>2015-06-02 09:15:27</td>\n",
              "      <td>1</td>\n",
              "      <td>[[Chris McIntosh, 0.8306], [Christopher Graham...</td>\n",
              "      <td>[http://www.computerweekly.com/news/4500247359...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>2015-12-09-000537</td>\n",
              "      <td>[ Starkman ] brings together the standards tha...</td>\n",
              "      <td>Jim Sleeper</td>\n",
              "      <td>[Q6198163]</td>\n",
              "      <td>2015-12-09 06:58:15</td>\n",
              "      <td>1</td>\n",
              "      <td>[[Jim Sleeper, 0.6759], [None, 0.2214], [Dean ...</td>\n",
              "      <td>[http://yaledailynews.com/blog/2015/12/09/visi...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>2015-08-12-028781</td>\n",
              "      <td>However, in the past we've had year-long perio...</td>\n",
              "      <td>Hamid Jafari</td>\n",
              "      <td>[Q29654671]</td>\n",
              "      <td>2015-08-12 16:55:20</td>\n",
              "      <td>1</td>\n",
              "      <td>[[Hamid Jafari, 0.9564], [None, 0.0436]]</td>\n",
              "      <td>[http://antiguaobserver.com/africa-marks-polio...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>2015-07-14-000994</td>\n",
              "      <td>[ Steven Universe ] is based on my younger bro...</td>\n",
              "      <td>Rebecca Sugar</td>\n",
              "      <td>[Q12569944]</td>\n",
              "      <td>2015-07-14 17:56:19</td>\n",
              "      <td>1</td>\n",
              "      <td>[[Rebecca Sugar, 0.8509], [None, 0.1237], [Zac...</td>\n",
              "      <td>[http://www.inquisitr.com/2251448/steven-unive...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>2015-08-27-031752</td>\n",
              "      <td>However, in violation of the rules these lands...</td>\n",
              "      <td>Tirath Singh Rawat</td>\n",
              "      <td>[Q27947626]</td>\n",
              "      <td>2015-08-27 14:05:46</td>\n",
              "      <td>2</td>\n",
              "      <td>[[Tirath Singh Rawat, 0.7008], [None, 0.2992]]</td>\n",
              "      <td>[http://economictimes.indiatimes.com/news/poli...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              quoteID  ... phase\n",
              "0   2015-08-31-000271  ...     E\n",
              "1   2015-12-08-029916  ...     E\n",
              "2   2015-09-10-000206  ...     E\n",
              "3   2015-07-23-032707  ...     E\n",
              "4   2015-10-04-000115  ...     E\n",
              "5   2015-06-19-016191  ...     E\n",
              "6   2015-08-31-000309  ...     E\n",
              "7   2015-09-21-030618  ...     E\n",
              "8   2015-12-10-000332  ...     E\n",
              "9   2015-06-18-018819  ...     E\n",
              "10  2015-08-21-000405  ...     E\n",
              "11  2015-09-13-014399  ...     E\n",
              "12  2015-09-16-000331  ...     E\n",
              "13  2015-12-17-031881  ...     E\n",
              "14  2015-12-15-000241  ...     E\n",
              "15  2015-10-29-035215  ...     E\n",
              "16  2015-12-21-000213  ...     E\n",
              "17  2015-12-17-031947  ...     E\n",
              "18  2015-08-28-000528  ...     E\n",
              "19  2015-09-25-029553  ...     E\n",
              "20  2015-08-25-000543  ...     E\n",
              "21  2015-03-22-011043  ...     E\n",
              "22  2015-12-13-000405  ...     E\n",
              "23  2015-03-25-025189  ...     E\n",
              "24  2015-09-30-000567  ...     E\n",
              "25  2015-03-08-011997  ...     E\n",
              "26  2015-10-25-000242  ...     E\n",
              "27  2015-04-06-017849  ...     E\n",
              "28  2015-07-06-000009  ...     E\n",
              "29  2015-04-14-023773  ...     E\n",
              "30  2015-08-17-000450  ...     E\n",
              "31  2015-04-25-012511  ...     E\n",
              "32  2015-10-23-000509  ...     E\n",
              "33  2015-10-04-014109  ...     E\n",
              "34  2015-10-02-000466  ...     E\n",
              "35  2015-10-04-014110  ...     E\n",
              "36  2015-11-13-000427  ...     E\n",
              "37  2015-07-21-029023  ...     E\n",
              "38  2015-07-22-000568  ...     E\n",
              "39  2015-12-31-019814  ...     E\n",
              "40  2015-08-17-000497  ...     E\n",
              "41  2015-03-26-025269  ...     E\n",
              "42  2015-08-27-000768  ...     E\n",
              "43  2015-10-23-031166  ...     E\n",
              "44  2015-12-01-000551  ...     E\n",
              "45  2015-06-02-000948  ...     E\n",
              "46  2015-12-09-000537  ...     E\n",
              "47  2015-08-12-028781  ...     E\n",
              "48  2015-07-14-000994  ...     E\n",
              "49  2015-08-27-031752  ...     E\n",
              "\n",
              "[50 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b84b568e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "461edfaf-4ab6-4726-da4b-aa1d7879855c"
      },
      "source": [
        "first_chunk.iloc[1]['speaker'] == 'None'"
      ],
      "id": "b84b568e",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4b4867b"
      },
      "source": [
        "We will replace these string 'None' by proper Nan to ease the processing with pandas."
      ],
      "id": "c4b4867b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1420adca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50c412c2-acf7-4b89-cf25-8c52706b0092"
      },
      "source": [
        "first_chunk['quoteID'].describe()"
      ],
      "id": "1420adca",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count                    50\n",
              "unique                   50\n",
              "top       2015-08-31-000271\n",
              "freq                      1\n",
              "Name: quoteID, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4afa184e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0663665b-675b-4e7b-f91a-feade7928900"
      },
      "source": [
        "first_chunk['quotation'].describe()"
      ],
      "id": "4afa184e",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count                                                    50\n",
              "unique                                                   50\n",
              "top       ... a great day for veterans here in Littleton...\n",
              "freq                                                      1\n",
              "Name: quotation, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abbb9236",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fadf4468-9b5a-4cf1-dba1-40c71bb2b9d5"
      },
      "source": [
        "first_chunk['speaker'].describe()"
      ],
      "id": "abbb9236",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count       50\n",
              "unique      38\n",
              "top       None\n",
              "freq        13\n",
              "Name: speaker, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ab03e75e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "011c6594-6c76-456f-f550-2401264d2663"
      },
      "source": [
        "first_chunk['qids'].describe()"
      ],
      "id": "ab03e75e",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count     50\n",
              "unique    38\n",
              "top       []\n",
              "freq      13\n",
              "Name: qids, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b5a7080",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "067cd236-a41f-4496-a797-c9407f60be8a"
      },
      "source": [
        "first_chunk['date'].describe()"
      ],
      "id": "2b5a7080",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: FutureWarning: Treating datetime data as categorical rather than numeric in `.describe` is deprecated and will be removed in a future version of pandas. Specify `datetime_is_numeric=True` to silence this warning and adopt the future behavior now.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count                      50\n",
              "unique                     50\n",
              "top       2015-08-31 02:10:00\n",
              "freq                        1\n",
              "first     2015-03-08 06:11:56\n",
              "last      2015-12-31 17:05:06\n",
              "Name: date, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e0a1ae9"
      },
      "source": [
        "We will need to transform the date column type to a proper datetime type."
      ],
      "id": "1e0a1ae9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "368f14d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc26d4d3-8a16-4c66-8fb7-2a92c6df2378"
      },
      "source": [
        "first_chunk['numOccurrences'].describe()"
      ],
      "id": "368f14d9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    50.000000\n",
              "mean      2.200000\n",
              "std       4.961073\n",
              "min       1.000000\n",
              "25%       1.000000\n",
              "50%       1.000000\n",
              "75%       1.750000\n",
              "max      35.000000\n",
              "Name: numOccurrences, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb6d5255",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "6de4bc4d-bdd2-42d2-82c6-32bb61790e2a"
      },
      "source": [
        "first_chunk['numOccurrences'].hist()\n",
        "plt.show()"
      ],
      "id": "fb6d5255",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14b2759f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7f05b6b-6cd6-411b-e1a7-91f9a241d883"
      },
      "source": [
        "first_chunk['probas'].describe()"
      ],
      "id": "14b2759f",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count                                                    50\n",
              "unique                                                   50\n",
              "top       [[Jeanne Shaheen, 0.742], [None, 0.2359], [Kel...\n",
              "freq                                                      1\n",
              "Name: probas, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94ded18b"
      },
      "source": [
        "The probas feature already contains python list."
      ],
      "id": "94ded18b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c855b0ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76132d81-4e8b-40f2-95f5-3cf781397628"
      },
      "source": [
        "first_chunk['urls'].describe()"
      ],
      "id": "c855b0ae",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count                                                    50\n",
              "unique                                                   50\n",
              "top       [http://www.unionleader.com/article/20150831/N...\n",
              "freq                                                      1\n",
              "Name: urls, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3f796619",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba58dfab-3122-4cca-b061-bc65fdb6d92b"
      },
      "source": [
        "first_chunk['phase'].describe()"
      ],
      "id": "3f796619",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count     50\n",
              "unique     1\n",
              "top        E\n",
              "freq      50\n",
              "Name: phase, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a45113a"
      },
      "source": [
        "It seems that the column 'phase' does contain much information since it is always the same value (to be confirmed on the entire dataset). If it is the case, we can just drop it."
      ],
      "id": "6a45113a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e57NwgjseUci"
      },
      "source": [
        "for year in range(2015, 2020 + 1):\n",
        "  count_nb_rows_in_file(quotes[year])"
      ],
      "id": "e57NwgjseUci",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "206ceca9"
      },
      "source": [
        "## Cleaning\n",
        "As our project idea is based not only on the quote but also on the author, we decided to drop the rows that were having None as speaker or the rows where the speaker has a probability less than 0.5."
      ],
      "id": "206ceca9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba1bbee5"
      },
      "source": [
        "def cast_date_remove_authors_low_proba(df_chunk, nb_rows_dropped):\n",
        "  \"\"\"\n",
        "    cast the date and remove authors with low probability\n",
        "\n",
        "    Parameters:\n",
        "      df_chunk :        chunk of the data frame \n",
        "      nb_rows_dropped : number of rows dropped\n",
        "\n",
        "    Returns:\n",
        "      df_chunk :        chunk of the data frame updated\n",
        "      nb_rows_dropped : number of rows dropped updated\n",
        "  \"\"\"\n",
        "  def create_col_author_highest_proba(row):\n",
        "    max_proba = -1.0\n",
        "    max_author = None\n",
        "    \n",
        "    for author, proba in row['probas']:\n",
        "      if float(proba) > max_proba:\n",
        "        max_proba = float(proba)\n",
        "        max_author = author\n",
        "            \n",
        "    return max_author, max_proba\n",
        "      \n",
        "      \n",
        "  # Cast the date column to datetime\n",
        "  df_chunk['date'] = pd.to_datetime(df_chunk['date'])\n",
        "  \n",
        "  # Cast the string 'None' for the speaker column to proper np.nan\n",
        "  df_chunk['speaker'] = df_chunk['speaker'].replace('None', np.nan)\n",
        "  \n",
        "  # Drop all the rows where the author is nan\n",
        "  df_chunk.dropna(subset=['speaker'], inplace=True)\n",
        "  \n",
        "  tmp = pd.DataFrame()\n",
        "  # Create 2 new columns with author, proba that has the highest proba\n",
        "  tmp[['author_highest_proba', 'highest_proba']] = df_chunk.apply(create_col_author_highest_proba, axis=1, result_type='expand')\n",
        "  \n",
        "  # Check if for some rows the author is not the author with the highest proba\n",
        "  if not df_chunk['speaker'].equals(tmp['author_highest_proba']):\n",
        "    print('========================================================================')\n",
        "    print('The column \"speaker\" is not equal to the column \"author_highest_proba\" !')\n",
        "    print('========================================================================')\n",
        "    \n",
        "    # Print where the 2 columns are different\n",
        "    print(df_chunk[np.argwhere(not df_chunk['speaker'].equals(tmp['author_highest_proba']))])\n",
        "      \n",
        "  # Drop the rows where the highest proba is < 0.5\n",
        "  mask = tmp['highest_proba'] < 0.5\n",
        "  nb_rows_dropped += mask.sum()\n",
        "  df_chunk = df_chunk[~mask]\n",
        "  \n",
        "  return df_chunk, nb_rows_dropped"
      ],
      "id": "ba1bbee5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-jZnAGXUM9d"
      },
      "source": [
        "Number of rows dropped in quotes 2015 : 662490<br>\n",
        "Number of rows dropped in quotes 2016 : 453279<br>\n",
        "Number of rows dropped in quotes 2017 : 859026<br>\n",
        "Number of rows dropped in quotes 2018 : 883951<br>\n",
        "Number of rows dropped in quotes 2019 : 695779<br>\n",
        "Number of rows dropped in quotes 2020 : 160320<br>\n",
        "\n",
        "Total number of records in file \"cleaned-quotes-2015\" : 13195548<br>\n",
        "Total number of records in file \"cleaned-quotes-2016\" : 8653435<br>\n",
        "Total number of records in file \"cleaned-quotes-2017\" : 16474272<br>\n",
        "Total number of records in file \"cleaned-quotes-2018\" : 16786468<br>\n",
        "Total number of records in file \"cleaned-quotes-2019\" : 13487515<br>\n",
        "Total number of records in file \"cleaned-quotes-2020\" : 3283285<br>"
      ],
      "id": "p-jZnAGXUM9d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iF8_GMcZwIck"
      },
      "source": [
        "for year in range(2015, 2020 + 1):\n",
        "  filter_file_and_save(cast_date_remove_authors_low_proba, quotes_name[year], quotes[year], cleaned_quotes[year])"
      ],
      "id": "iF8_GMcZwIck",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_0HRt9r4nQP"
      },
      "source": [
        "## Sampling the dataset\n",
        "Since the original dataset is very large, it is quite cumbersome to do our first analyses on it because it takes a lot of time. Therefore to get a first glance at the data, test our functions, etc, we sample the original dataset to obtain a much smaller one.\n",
        "\n",
        "\n",
        "Now arises the question on how should we sample it ? Should we just sample uniformly at random a fixed number of quotes per year ? Should we look for biases in the data as seen in the \"Observation studies\" lecture and sample the dataset accordingly ? For example, we could look at the proportion of women/men authors in each year and decide to balance the samples according to this. However, this would require to merge the dataset with the Wikidata parquet file and so on. So for now, we decided to start by sampling uniformly at random 500'000 quotes from each year from the cleaned dataset we created above. Since the sampling is random, this should conserve the biases of the origininal dataset and therefore we will be able to detect them faster (in terms of computational time). Obviously, we will run our analyses on the full dataset once in the end to get more representative results."
      ],
      "id": "v_0HRt9r4nQP"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yc_MV3ga73qJ"
      },
      "source": [
        "def sample_dataset(input_file, output_file, nb_samples, seed=1, chunksize=750_000):\n",
        "  if(isfile(input_file) and not isfile(output_file)):\n",
        "    # First need to get back the quoteID of all the rows in the file\n",
        "    quoteIDs = []\n",
        "    with pd.read_json(input_file, lines=True, compression='bz2', chunksize=chunksize) as df_reader:\n",
        "      for df_chunk in tqdm(df_reader):\n",
        "        quoteIDs += df_chunk['quoteID'].tolist()\n",
        "\n",
        "    print('==> quoteIDs processed')\n",
        "\n",
        "    # Choose nb_samples uniformly at random\n",
        "    rng = np.random.default_rng(seed=seed)\n",
        "    quoteIDs_samples = rng.choice(quoteIDs, nb_samples, replace=False)\n",
        "\n",
        "    with pd.read_json(input_file, lines=True, compression='bz2', chunksize=chunksize) as df_reader:\n",
        "      with bz2.open(output_file, 'wb') as out_file:\n",
        "        for df_chunk in tqdm(df_reader):\n",
        "          # Keep only chosen rows\n",
        "          df_result = df_chunk.loc[df_chunk['quoteID'].isin(quoteIDs_samples), :]\n",
        "          \n",
        "          # Write result chunk to file\n",
        "          write_df_chunk_to_file(df_result, out_file)\n",
        "\n",
        "    print(f'==> Succesfully sampled {nb_samples} out of {len(quoteIDs)} from file \"{input_file}\"')"
      ],
      "id": "yc_MV3ga73qJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zYIxwHADI6G"
      },
      "source": [
        "nb_samples = 300_000\n",
        "for year in range(2015, 2020 + 1):\n",
        "  print(f'==> {year} start')\n",
        "  sample_dataset(cleaned_quotes[year], sampled_quotes[year], nb_samples)\n",
        "  print(f'==> {year} processed')"
      ],
      "id": "0zYIxwHADI6G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zZR9X7rLqga"
      },
      "source": [
        "To ease the processing on the sampled dataset, we will merge the 6 sampled files into a single one."
      ],
      "id": "2zZR9X7rLqga"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Feyq1FtALH77",
        "collapsed": true
      },
      "source": [
        "# Now let's merge these 6 sampled files\n",
        "apply_to_all_stream_and_save(lambda df,args: (df,args),\n",
        "                            sampled_quotes,\n",
        "                            sampled_quotes_all)"
      ],
      "id": "Feyq1FtALH77",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9aJyhq8B3vP"
      },
      "source": [
        "## Add Wikidata information\n",
        "\n",
        "As in our project we need the age, sex and nationality, we decided to add the information from wikidata related to this directly in the dataset."
      ],
      "id": "K9aJyhq8B3vP"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sn-wM_4CRbVt"
      },
      "source": [
        "def get_labels_from_QID():\n",
        "  \"\"\"\n",
        "    return a map from QID to labels\n",
        "\n",
        "    Returns:\n",
        "      labels_from_QID : a map from QID to labels\n",
        "  \"\"\"\n",
        "  labels_from_QID = {}\n",
        "  with pd.read_csv(wikidata_labels_quotebank, compression='bz2', chunksize=1_000_000) as df_reader:\n",
        "    for chunk in df_reader:\n",
        "      for index,row in chunk.iterrows():\n",
        "        labels_from_QID[str(row[\"QID\"])] = str(row[\"Label\"])\n",
        "  return labels_from_QID\n",
        "\n",
        "def get_Label(QIDs, labels_from_QID):\n",
        "  \"\"\"\n",
        "    get the label from QIDs\n",
        "\n",
        "    Parameters:\n",
        "      QIDs :            list of QIDs\n",
        "      labels_from_QID : map from QID to labels\n",
        "\n",
        "    Returns:\n",
        "      labels :          list of labels\n",
        "  \"\"\"\n",
        "  if(str(QIDs) == 'None'):\n",
        "    return None\n",
        "  return [labels_from_QID[str(x)] for x in QIDs if str(x) in labels_from_QID]\n",
        "\n",
        "def first_if_not_None(element):\n",
        "  \"\"\"\n",
        "    return the first element or None\n",
        "\n",
        "    Parameters:\n",
        "      element : list of elements or None\n",
        "\n",
        "    Returns:\n",
        "      labels :  the first element or None\n",
        "  \"\"\"\n",
        "  if (str(x) == 'None'):\n",
        "    return None\n",
        "  return str(x[0])[1:-10]\n",
        "\n",
        "def get_speaker_atribut():\n",
        "  \"\"\"\n",
        "    return a map from QID to speaker atribut\n",
        "\n",
        "    Returns:\n",
        "      speaker_atribut : a map from QID to speaker atribut\n",
        "  \"\"\"\n",
        "  labels_from_QID = get_labels_from_QID()\n",
        "  speaker_atribut = {}\n",
        "\n",
        "  for file in wikidata_speaker_parquet:\n",
        "    df = pq.read_pandas(file)\n",
        "    for i in range(len(df['aliases'])):\n",
        "\n",
        "      result = {\n",
        "        'date_of_birth' : first_if_not_None(df['date_of_birth'][i]),\n",
        "        'gender' : get_Label(df['gender'][i], labels_from_QID),\n",
        "        'nationality' : get_Label(df['nationality'][i], labels_from_QID)\n",
        "      }\n",
        "      if(result['date_of_birth'] != None or result['gender'] != None or result['nationality'] != None):\n",
        "        speaker_atribut[str(df['id'][i])] = result\n",
        "\n",
        "  return speaker_atribut"
      ],
      "id": "Sn-wM_4CRbVt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMLBBU1BA4F1"
      },
      "source": [
        "def add_speaker_atribut(df_chunk, args):\n",
        "  \"\"\"\n",
        "    add speaker atribut to the data frame \n",
        "\n",
        "    Parameters:\n",
        "      df_chunk :          chunk of the data frame\n",
        "      args :\n",
        "        speaker_atribut:  map from QID to speaker atribut\n",
        "        drops:            number of rows dropped\n",
        "\n",
        "    Returns:\n",
        "      new_df_chunk :      chunk of the data frame updated\n",
        "      args :\n",
        "        speaker_atribut:  map from QID to speaker atribut\n",
        "        drops:            number of rows dropped updated\n",
        "  \"\"\"\n",
        "  speaker_atribut, drops = args\n",
        "\n",
        "  new_df_chunk = df_chunk[df_chunk['qids'].apply(lambda row : (str(row[0]) in speaker_atribut))]\n",
        "\n",
        "  def compute_age(row):\n",
        "    \"\"\"\n",
        "    compute the age of the speaker\n",
        "\n",
        "    Parameters:\n",
        "      row : row of the data frame\n",
        "\n",
        "    Returns:\n",
        "      age : age of the speaker\n",
        "  \"\"\"\n",
        "    date_of_birth = speaker_atribut[str(row['qids'][0])]['date_of_birth']\n",
        "    date = row['date']\n",
        "    if(date == None or date_of_birth == None):\n",
        "      return None\n",
        "\n",
        "    \n",
        "    date_of_birth = date_of_birth\\\n",
        "      .replace('-00', '-01')\\\n",
        "      .replace('-02-31', '-02-29')\\\n",
        "      .replace('-06-31', '-06-30')\n",
        "\n",
        "    date = datetime.strptime(str(date),'%Y-%m-%d %H:%M:%S')\n",
        "    try:\n",
        "      date_of_birth = datetime.strptime(date_of_birth,'%Y-%m-%d')\n",
        "      return (date - date_of_birth).days//365.25\n",
        "    except ValueError:\n",
        "      try:\n",
        "        #arrive there if the 29 february is not a valid date for this year\n",
        "        date_of_birth = date_of_birth\\\n",
        "          .replace('-02-29', '-02-28')\n",
        "        date_of_birth = datetime.strptime(date_of_birth,'%Y-%m-%d')\n",
        "        return (date - date_of_birth).days//365.25\n",
        "      except ValueError:\n",
        "        #unknown exception\n",
        "        print()\n",
        "        print('ValueError:')\n",
        "        print('date_of_birth', date_of_birth)\n",
        "        print()\n",
        "        return None\n",
        "\n",
        "  new_df_chunk['speaker_age'] = new_df_chunk.apply(lambda row : compute_age(row), axis=1)\n",
        "  new_df_chunk['speaker_gender'] = new_df_chunk.apply(lambda row : speaker_atribut[str(row['qids'][0])]['gender'], axis=1)\n",
        "  new_df_chunk['speaker_nationality'] = new_df_chunk.apply(lambda row : speaker_atribut[str(row['qids'][0])]['nationality'], axis=1)\n",
        "\n",
        "  drops += len(df_chunk) - len(new_df_chunk)\n",
        "  return new_df_chunk, (speaker_atribut, drops)\n",
        "speaker_atribut = {}"
      ],
      "id": "BMLBBU1BA4F1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeTFZWB2ATth"
      },
      "source": [
        "Number of rows dropped in quotes 2015 : 183692<br>\n",
        "Number of rows dropped in quotes 2016 : 125189<br>\n",
        "Number of rows dropped in quotes 2017 : 251961<br>\n",
        "Number of rows dropped in quotes 2018 : 265048<br>\n",
        "Number of rows dropped in quotes 2019 : 218068<br>\n",
        "Number of rows dropped in quotes 2020 : 56564<br>\n",
        "\n",
        "Total number of records in file \"speaker-quotes-2015\" : 13011856<br>\n",
        "Total number of records in file \"speaker-quotes-2016\" : 8528246<br>\n",
        "Total number of records in file \"speaker-quotes-2017\" : 16222311<br>\n",
        "Total number of records in file \"speaker-quotes-2018\" : 16521420<br>\n",
        "Total number of records in file \"speaker-quotes-2019\" : 13269447<br>\n",
        "Total number of records in file \"speaker-quotes-2020\" : 3226721<br>"
      ],
      "id": "HeTFZWB2ATth"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gByRC3cDmqIr"
      },
      "source": [
        "for year in range(2015, 2020 + 1):\n",
        "  if(isfile(cleaned_quotes[year]) and not isfile(speaker_quotes[year])):\n",
        "    if(len(speaker_atribut) == 0):\n",
        "      speaker_atribut = get_speaker_atribut()\n",
        "    print(f'==> {year} start')\n",
        "    args = apply_to_stream_and_save(add_speaker_atribut, cleaned_quotes[year], speaker_quotes[year], args=(speaker_atribut, 0), chunksize=500_000)\n",
        "    _, drops = args\n",
        "    print(f'==> {year} processed, row deleted: {drops}')"
      ],
      "id": "gByRC3cDmqIr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdIfuNIcxi_p"
      },
      "source": [
        "for year in range(2015, 2020 + 1):\n",
        "  if(isfile(sampled_quotes[year]) and not isfile(sampled_speaker_quotes[year])):\n",
        "    if(len(speaker_atribut) == 0):\n",
        "      speaker_atribut = get_speaker_atribut()\n",
        "    print(f'==> {year} start')\n",
        "    args = apply_to_stream_and_save(add_speaker_atribut, sampled_quotes[year], sampled_speaker_quotes[year], args=(speaker_atribut, 0), chunksize=500_000)\n",
        "    _, drops = args\n",
        "    print(f'==> {year} processed, row deleted: {drops}')"
      ],
      "id": "FdIfuNIcxi_p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ahvox8UyRbsD"
      },
      "source": [
        "# Now let's merge these 6 sampled files\n",
        "apply_to_all_stream_and_save(lambda df,args: (df,args),\n",
        "                            sampled_speaker_quotes,\n",
        "                            sampled_speaker_quotes_all)"
      ],
      "id": "Ahvox8UyRbsD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tb_j4Gy70Ltm"
      },
      "source": [
        "# Data exploration\n",
        "\n",
        "In this part, we choose to plot relevant distributions for our project. Since our project involves a temporal component, we choose to plot and explore the data year by year on our sampled dataset. \n",
        "\n",
        "Some remarks: \n",
        "- For the histogram of the number of quotes per age, we limit ourselves to people whose ages are below 120.\n",
        "- To plot a meaningful representation of the number of quotes per country, we keep only the quotes whose author has only one nationality. \n",
        "\n",
        "Our initial objective was to give a year by year analysis but since the plots are really close between the year, we think that it is more pertinent to give a global explanation rather than repeating ourselves. Nevertheless, we will point out some of the differences. "
      ],
      "id": "Tb_j4Gy70Ltm"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ecr6puN-3NCF"
      },
      "source": [
        "plt_aspect=2\n",
        "plt.figure(figsize=(12,9))\n",
        "\n",
        "def describe(df, year):\n",
        "  \"\"\"\n",
        "    describe the distribution of the data frame\n",
        "\n",
        "    Parameters:\n",
        "      df :    the data frame of a specific year\n",
        "      year :  the year\n",
        "  \"\"\"\n",
        "  print(year, ':')\n",
        "  print(df.describe())\n",
        "\n",
        "def num_occurences(df, year):\n",
        "  \"\"\"\n",
        "    plot the distribution of the number of occurences per quotes\n",
        "\n",
        "    Parameters:\n",
        "      df :    the data frame of a specific year\n",
        "      year :  the year\n",
        "  \"\"\"\n",
        "  ax = sns.displot(data=df, x='numOccurrences', log_scale=(True, True), aspect=plt_aspect)\n",
        "  plt.suptitle(f'Histogram of number of quotes per number of occurences ({year})')\n",
        "  ax.set(xlabel=\"Number of occurences\",ylabel=\"Number of quotes\")\n",
        "  plt.show()\n",
        "\n",
        "def quotes_by_author(df, year):\n",
        "  \"\"\"\n",
        "    plot the distribution of the number of quotes by author\n",
        "\n",
        "    Parameters:\n",
        "      df :    the data frame of a specific year\n",
        "      year :  the year\n",
        "  \"\"\"\n",
        "  # Count the number of quote per author\n",
        "  df['Author'] = df.apply(lambda row : row['qids'][0], axis=1)\n",
        "  var = df.groupby('Author').agg(count=pd.NamedAgg(column=\"Author\", aggfunc=\"count\"))\n",
        "  # Display the distribution\n",
        "  ax =sns.displot(data=var, x='count', log_scale=(True, True), aspect=plt_aspect)\n",
        "  ax.set(xlabel=\"Quotes per author\",ylabel=\"Number of author\")\n",
        "  plt.suptitle(f'Histogram of number of quotes by Author ({year})')\n",
        "  plt.show()\n",
        "\n",
        "def quotes_per_ages(df, year):\n",
        "  \"\"\"\n",
        "    plot a histogram of the number of quotes by ages\n",
        "\n",
        "    Parameters:\n",
        "      df :    the data frame of a specific year\n",
        "      year :  the year\n",
        "  \"\"\"\n",
        "  df = df[df['speaker_age'] < 120]\n",
        "  # Create a histogram of the number of quotes per age\n",
        "  ax = sns.displot(data=df[pd.notna(df['speaker_age'])], x='speaker_age', log_scale=(False, True), bins=120, aspect=plt_aspect)\n",
        "  ax.set(xlabel=\"Ages\",ylabel=\"Count per ages\")\n",
        "  plt.suptitle(f'Number of quotes per ages ({year})')\n",
        "  plt.show()\n",
        "\n",
        "def to_gender_string(gender):\n",
        "  \"\"\"\n",
        "    return the gender in string\n",
        "\n",
        "    Parameters:\n",
        "      gender :        list of gender\n",
        "\n",
        "    Returns:\n",
        "      gender_string : gender in string\n",
        "  \"\"\"\n",
        "  if gender == None :\n",
        "    return \"None\"\n",
        "  s = \"\"\n",
        "  for v in set(gender) :\n",
        "    s += \", \" + v\n",
        "  return s[2:]\n",
        "\n",
        "def to_set(x):\n",
        "  \"\"\"\n",
        "    return the set of the list\n",
        "\n",
        "    Parameters:\n",
        "      x :   list\n",
        "\n",
        "    Returns:\n",
        "      set : set\n",
        "  \"\"\"\n",
        "  if x == None :\n",
        "    return None\n",
        "  return set(x)\n",
        "\n",
        "def quotes_per_sex(df, year):\n",
        "  \"\"\"\n",
        "    plot a distribution of the number of quotes per sex\n",
        "\n",
        "    Parameters:\n",
        "      df :    the data frame of a specific year\n",
        "      year :  the year\n",
        "  \"\"\"\n",
        "  # Create new columns containing the set of sex of the author\n",
        "  df['speaker_gender_set'] = df['speaker_gender'].apply(to_gender_string)\n",
        "  ax = sns.displot(data = df, x = 'speaker_gender_set' , log_scale = (False,True), aspect=plt_aspect)\n",
        "  plt.xticks(rotation=90)\n",
        "  ax.set(ylabel=\"Quotes per sex\")\n",
        "  plt.suptitle(f'Bar plot of number of quotes by gender  ({year})')\n",
        "  plt.show()\n",
        "\n",
        "def quotes_per_nationality(df, year):\n",
        "  \"\"\"\n",
        "    plot a distribution of the number of quotes per nationality\n",
        "\n",
        "    Parameters:\n",
        "      df :    the data frame of a specific year\n",
        "      year :  the year\n",
        "  \"\"\"\n",
        "  # Display the total number of quotes and the number of quotes with an author that has only one nationality\n",
        "  df['speaker_nationality_set'] = df['speaker_nationality'].apply(lambda x: to_set(x))\n",
        "  print(f\"Total number of quotes: {len(df)}\")  \n",
        "  print(f\"Number of quotes with only one nationality: {df['speaker_nationality_set'].apply(lambda x: (x is not None) and (len(x)==1)).sum()}\")\n",
        "\n",
        "  # Counte the number of quotes per nationality where the author has only one nationality \n",
        "  one_nat = pd.DataFrame()\n",
        "  one_nat['nationality_set'] = df['speaker_nationality_set'].apply(lambda x: list(x)[0] if ((x is not None) and (len(x)==1)) else np.nan).dropna()\n",
        "  one_nat = one_nat.groupby('nationality_set').agg(count=pd.NamedAgg(column=\"nationality_set\", aggfunc=\"count\"))\n",
        "\n",
        "  # Plot the number of quotes per nationality histogram\n",
        "  ax = sns.displot(data = one_nat, x = 'count' , log_scale = (True, True), bins=50, aspect=plt_aspect)\n",
        "  plt.xticks(rotation=90)\n",
        "  ax.set(xlabel=\"Number of quotes\", ylabel=\"Number of nationality\")\n",
        "  plt.suptitle(f'Number of nationality by number of ({year})')\n",
        "  plt.show()\n",
        "\n",
        "def quotes_per_day(df, year):\n",
        "  \"\"\"\n",
        "    plot a distribution of the number of quotes per day of year\n",
        "\n",
        "    Parameters:\n",
        "      df :    the data frame of a specific year\n",
        "      year :  the year\n",
        "  \"\"\"\n",
        "  # Compute the number of quotes per day of year \n",
        "  day_of_year = pd.DataFrame()\n",
        "  day_of_year['day_of_year'] = df['date'].apply(lambda x: x.day_of_year)\n",
        "\n",
        "  # Plot the line plot for the full year\n",
        "  from matplotlib import rcParams\n",
        "  rcParams['figure.figsize'] = 25,4\n",
        "  ax = sns.histplot(data=day_of_year, x='day_of_year', binwidth=1)\n",
        "  ax.set(xlabel=\"Day of year\",ylabel=\"Count per day\")\n",
        "  plt.suptitle(f'Number of quotes per day ({year})')\n",
        "  plt.show()\n",
        "\n",
        "def describe_year(df, year):\n",
        "  print(f\"==== Exploration sampled quotes for year {year} ====\")\n",
        "  # Statistical description of this year data.\n",
        "  print(\"\\n\\nStatistical summary\\n\\n\")\n",
        "  print(df.describe())\n",
        "  # Display all the plot for the current year\n",
        "  print(\"\\n\\nVisualisations\\n\\n\")\n",
        "  numOccurences(df, year)\n",
        "  quotesByAuthor(df, year)\n",
        "  quotesPerAges(df[df['speaker_age'] < 120], year)\n",
        "  quotesPerSex(df, year)\n",
        "  quotesPerNationality(df, year)\n",
        "  quotes_per_day(df, year)\n",
        "  \n",
        "def plot_all_years(func, df_zip):\n",
        "  for df, year in df_zip:\n",
        "    func(df, year)\n",
        "    print()\n"
      ],
      "id": "Ecr6puN-3NCF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p63mB2drmTV5"
      },
      "source": [
        "df_zip = [(pd.read_json(sampled_speaker_quotes[year], lines=True, compression='bz2'), year) for year in range(2015, 2020 + 1)]"
      ],
      "id": "p63mB2drmTV5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DO1z4p7aFC7N"
      },
      "source": [
        "plot_all_years(describe, df_zip)"
      ],
      "id": "DO1z4p7aFC7N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-9U3ZnNFDCr"
      },
      "source": [
        ""
      ],
      "id": "u-9U3ZnNFDCr"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDQ6F2zoobPV"
      },
      "source": [
        "plot_all_years(num_occurences, df_zip)"
      ],
      "id": "VDQ6F2zoobPV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ns6fmixtMNK"
      },
      "source": [
        "The first good insight we have is that both the number of occurences per quote and the number of quotes seems to follow a power low as they PDF as the histograms looks like a line in a log-log plot. We can also see that there are some extreme outliers in each of the plot."
      ],
      "id": "_ns6fmixtMNK"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fB0uvKfXq8ew"
      },
      "source": [
        "plot_all_years(quotes_by_author, df_zip)"
      ],
      "id": "fB0uvKfXq8ew",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GjPe1QFqTmR"
      },
      "source": [
        "Same as before, we can see that the number of quotes per author seems to follow a power law.\n"
      ],
      "id": "0GjPe1QFqTmR"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1_xc_8AqaeE"
      },
      "source": [
        "plot_all_years(quotes_per_ages, df_zip)"
      ],
      "id": "F1_xc_8AqaeE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFc3EHLYqZ2X"
      },
      "source": [
        "As explained earlier, to plot the histogram, we kept only the quoter with an age that between 0 and 120 since our project will mainly focus on person that are still alive (as we want to study the evolution). Keeping the age of people which are more than 120 years old will not give us any information. In the range 0 to 120 years old, we can see that the distribution is almost symmetrical with most of the data point concentrated between 20 and 100 years. The mode of the distribution is around 50 years. "
      ],
      "id": "VFc3EHLYqZ2X"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amwNfdxnq_2f"
      },
      "source": [
        "plot_all_years(quotes_per_sex, df_zip)"
      ],
      "id": "amwNfdxnq_2f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhEdWO0Dqez5"
      },
      "source": [
        "When we look at the plot representing the number of quotes per sex, we can see that most of the quotes are from men or women. We were really suprised to see how many different genders they are in our data but we can also see that they represent a minority."
      ],
      "id": "WhEdWO0Dqez5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pypwqY01qe9B"
      },
      "source": [
        "plot_all_years(quotes_per_nationality, df_zip)"
      ],
      "id": "pypwqY01qe9B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyVpWfKYrF38"
      },
      "source": [
        "As we will probably try to group author by nationality in our project, we plotted the number of quotes per nationality in each year. We can see that there is a kind of trend where : \n",
        "- There is a lot of nationality with only one quote.\n",
        "- Between 10 and 1000 quotes per nationality, the distribution is close to uniform. \n",
        "- Some nationalities between 1000 and 10'000 quotes  \n",
        "- One extreme nationality with around 100'000 quotes. "
      ],
      "id": "fyVpWfKYrF38"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cGNfPd4rYp4"
      },
      "source": [
        "plot_all_years(quotes_per_day, df_zip)"
      ],
      "id": "4cGNfPd4rYp4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCWy6834dJE2"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Finally, we plotted the number of quotes per day over the years. As we can see in the plot below, we can see a pattern where the number of quotes in the day of the week is higher than during the week-end. We can also note some interesting things: \n",
        "- The dates of the quotes stopped around the 110th day of 2020 which makes perfectly sense as Quotebank was created using data until April 2020. \n",
        "- A more interesting thing to note is that we can see that in 2015, in 2016 as well as the start of 2017, we have a really low number of quotes. Since we are working on a sample of the original dataset to perform this exploratory data analysis, we first thought that there might be a problem with our sampling method. However, since we sample quotes at random from the original dataset, it is very unlikely that we will get such a small number of quotes in these periods and a large number in the other. Furthermore, we can see that this doesn't happen in the later years (from February 2017 to 2020) where we can only see this weekly cycle.\n",
        "\n"
      ],
      "id": "OCWy6834dJE2"
    }
  ]
}